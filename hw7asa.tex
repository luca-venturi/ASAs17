\documentclass[a4paper,11pt]{article}

\usepackage{geometry}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage[font={small},labelfont={sf,bf}]{caption}
\usepackage{color}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{afterpage}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{bbm}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{stmaryrd}

\geometry{a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm,heightrounded, bindingoffset=5mm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{theo}[definition]{Theorem}
\newtheorem{prop}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{cor}[definition]{Corollary}
\newtheorem{ex}[definition]{Example}
\theoremstyle{remark}
\newtheorem{rem}[definition]{Remark}
\newtheorem{rem*}[definition]{}

\newcommand*\mcup{\mathbin{\mathpalette\mcupinn\relax}}
\newcommand*\mcupinn[2]{\vcenter{\hbox{$\mathsurround=0pt
  \ifx\displaystyle#1\textstyle\else#1\fi\bigcup$}}}
\newcommand*\mcap{\mathbin{\mathpalette\mcapinn\relax}}
\newcommand*\mcapinn[2]{\vcenter{\hbox{$\mathsurround=0pt
  \ifx\displaystyle#1\textstyle\else#1\fi\bigcap$}}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\parr}{(}{)}
\DeclarePairedDelimiter{\parq}{[}{]}
\DeclarePairedDelimiter{\parqq}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\bra}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\prodscal}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\expval}{\mathbb{E}}
\DeclareMathOperator*{\varval}{\mathrm{Var}}
\DeclareMathOperator*{\covval}{\mathrm{Cov}}

\begin{document}

\title{Applied Stochastic Analysis \\ Homework assignment 7}
\author{Luca Venturi}
\maketitle

\section*{Exercise 1}

Since every $Y_t^i$, $i=1,\dots,d$, is a linear combination of continuous independent (centered) Gaussian processes then every $Y_t^i$ is a continuous (centered) Gaussian process too. Hence, we only need to examine its covariance function. We have
\begin{align*}
\expval\parq{Y^i_sY^j_t} & = \sum_{k=1}^d\sum_{n=1}^d Q_{ik}Q_{jn}\expval\parq{W^k_sW^n_t} = (s\wedge t)\sum_{k=1}^d\sum_{n=1}^d Q_{ik}Q_{jn}\delta_{kn} \\ & = (s\wedge t)\sum_{k=1}^dQ_{ik}Q_{jk} = (s\wedge t)(Q\cdot Q^t)_{ij} = (s\wedge t)\delta_{ij}.
\end{align*}
The above concludes the proof that $\mathbf{Y}_t = (Y^1_t,\dots,Y^d_t)$ is a $d$-dimensional Brownian motion. 

\section*{Exercise 2}

\paragraph*{(a)}

First of all, we notice that, by definition of that $T_a$, $\tilde{B}_t$ is a.s. continuous. Hence, to show that $\tilde{B}_t$ is a B.m. (Brownian motion), it is sufficient to show that
$$
P(\tilde{B}_s \leq x, \tilde{B}_t \leq y) = P(B_s \leq x, B_t \leq y)
$$
for every $0\leq s\leq t$ and $x,y\in\mathbb{R}$ (i.e. that $(\tilde{B}_t,\tilde{B}_s)$ and $(B_t,B_s)$ have the same distribution). Let $\mathcal{F}_t= \sigma(B_s: 0\leq s \leq t)$. By the strong Markov property, we know that $W_t \doteq B_{t+T_a} - B_{T_a}$ is a B.m. independent by $\mathcal{F}_{T_a}$. Notice that 
$$
\tilde{W}_t \doteq \tilde{B}_{t+T_a} - \tilde{B}_{T_a} = 2a - B_{t+T_a} - B_{T_a} = B_{T_a} - B_{t+T_a} = -W_t,
$$
so $\tilde{W}_t$ is a B.m. independent by $\mathcal{F}_{T_a}$ too. We can write
\begin{align*}
B_t = B_t\mathbbm{1}_{\bra{t\leq T_a}} + (W_{t-T_a} + B_{T_a})\mathbbm{1}_{\bra{t> T_a}} \qquad \text{and} \qquad \tilde{B}_t = B_t\mathbbm{1}_{\bra{t\leq T_a}} + (\tilde{W}_{t-T_a} + B_{T_a})\mathbbm{1}_{\bra{t> T_a}}.
\end{align*}
Now 
\begin{align}
P(\tilde{B}_s \leq x, \tilde{B}_t \leq y) & = P(B_s \leq x, B_t \leq y, t\leq T_a) + P(B_s \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y, s\leq T_a<t) \notag \\ & + P(\tilde{W}_{s-T_a} + B_{T_a} \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y, T_a<s). \label{ex2:1}
\end{align}
We can evaluate the second term in the rhs as follows:
\begin{align*}
P(B_s \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y, s\leq T_a<t) & = \expval\parq{P(B_s \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y\,|\,\mathcal{F}_{T_a})\mathbbm{1}_{\bra{s\leq T_a<t}}}, 
\end{align*}
where
$$
P(B_s \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y\,|\,\mathcal{F}_{T_a}) = \tilde{f}(B_s,T_a)
$$
with
$$
\tilde{f}(z,r) \doteq \mathbbm{1}_{\bra{z \leq x}} P(\tilde{W}_{t-r} + a \leq y) = \mathbbm{1}_{\bra{z \leq x}} P(W_{t-r} + a \leq y) \doteq f(z,r).
$$
Hence
\begin{align}
P(B_s \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y, s\leq T_a<t) & = \expval\parq{f(B_s,T_a)\mathbbm{1}_{\bra{s\leq T_a<t}}} \notag \\ & = P(B_s \leq x, W_{t-T_a} + B_{T_a} \leq y, s\leq T_a<t).\label{ex2:2} 
\end{align}
Similarly one can prove that
\begin{equation}
P(\tilde{W}_{s-T_a} + B_{T_a} \leq x, \tilde{W}_{t-T_a} + B_{T_a} \leq y, T_a<s) = P(W_{s-T_a} + B_{T_a} \leq x, W_{t-T_a} + B_{T_a} \leq y, T_a<s) \label{ex2:3}.
\end{equation}
From (\ref{ex2:1}), (\ref{ex2:2}) and (\ref{ex2:3}) one gets that
\begin{align*}
P(\tilde{B}_s \leq x, \tilde{B}_t \leq y) & = P(B_s \leq x, B_t \leq y, t\leq T_a) + P(B_s \leq x, W_{t-T_a} + B_{T_a} \leq y, s\leq T_a<t) \\ & + P(W_{s-T_a} + B_{T_a} \leq x, W_{t-T_a} + B_{T_a} \leq y, T_a<s) \\ &  = P(B_s \leq x, B_t \leq y).
\end{align*}
This completes the proof that $\tilde{B}_t$ is a B.m.

\paragraph*{(b)}

It holds that
$$
P(M_t>a) = P(M_t>a,B_t>a) + P(M_t>a,B_t\leq a) = P(B_t>a) + P(M_t>a,B_t\leq a),
$$
since $B_t>a$ implies $M_t = \sup_{s\leq t}B_s\geq B_t>a$.

\paragraph*{(c)}

Since $\bra{M_t > a} = \bra{T_a\leq t}$ and if $T_a\leq t$ then $\tilde{B}_t = 2a - B_t$, then
$$
P(M_t > a, B_t \leq a) = P(T_a\leq t, 2a - B_t \geq a) = P(T_a\leq t, \tilde{B}_t \geq a) = P(\tilde{B}_t \geq a),
$$ 
since $\bra{T_a> t, \tilde{B}_t \geq a} = \bra{T_a> t, B_t \geq a} = \emptyset$.

\paragraph*{(d)}

Thanks to part (a), (b) and (c) we have that
$$
P(M_t>a) =  P(B_t>a) + P(\tilde{B}_t \geq a) = 2\int_a^\infty\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}\,dx = \int_a^\infty\sqrt{\frac{2}{\pi t}}e^{-x^2/2t}\,dx.
$$

\section*{Exercise 3}

Let
$$
Y^n_t = \frac{1}{\sqrt{n}}S_{\floor{nt}} + \frac{1}{\sqrt{n}}(nt - \floor{nt})X_{\floor{nt}+1}.
$$
By Donkser's theorem it holds that $Y^n\Rightarrow B$ as $n\to\infty$, where $Y^n = (Y^n_t)_{t\geq0}$ and $B = (B_t)_{t\geq0}$ is a B.m. In particular this implies that
\begin{equation}\label{ex3}
\sup_{t\in [0,1]} Y^n_t \quad\Rightarrow\quad M_1 = \sup_{t\in[0,1]} B_t
\end{equation}
as $n\to\infty$. We notice that
\begin{align*}
\sup_{t\in [0,1]} Y^n_t & = \frac{1}{\sqrt{n}}\sup_{t\in[0,1]}(S_{\floor{nt}} + (nt - \floor{nt})X_{\floor{nt}+1}) \\ & = \frac{1}{\sqrt{n}}\sup\bra{S_i + tX_{i+1}, i=0,\dots,n-1, t\in[0,1], S_n} = \frac{1}{\sqrt{n}}\sup_{i=0,\dots,n}S_n = \frac{1}{\sqrt{n}}G_n.
\end{align*}
Therefore, (\ref{ex3}) tells us that (heuristically), for $n$ large, $G_n$ is distributed as $\sqrt{n}M_1$. Noticing that exercise 2 tells us that $\sqrt{n}M_1 \sim \sqrt{n}\abs{B_1}$, we can (heuristically) say that
$$
\expval\parq{G_n} \simeq \expval\parq{\sqrt{n}\abs{B_1}} = \sqrt{\frac{2}{\pi n}}\int_0^\infty xe^{-x^2/2n}\,dx =\sqrt{\frac{2n}{\pi}}
$$
and
$$
\varval(G_n) = \expval\parq{G_n^2} - \expval\parq{G_n}^2 \simeq n\expval\parq{B_1^2} - \frac{2n}{\pi} =  n\parr*{1-\frac{2}{\pi}}
$$
for $n$ large.

\section*{Exercise 4}

\paragraph*{(a)}

We showed in class that
\begin{equation}\label{ex4:quad_var_conv}
Q_t^\sigma(W_t) \xrightarrow{L^2} t \qquad \text{as } \abs{\sigma}\to0.
\end{equation}
Hence it is sufficient to notice that
\begin{align*}
Q_t^\sigma(aW_t+bt) & = \sum_{i=0}^{n-1}\abs{aW_{t_{i+1}}+bt_{i+1}-aW_{t_i}-bt_i}^2 = \sum_{i=0}^{n-1}\abs{a(W_{t_{i+1}}-W_{t_i})+b(t_{i+1}-t_i)}^2 \\ & = a^2Q_t^\sigma(W_t) + ab\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})(t_{i+1}-t_i) + b^2\sum_{i=0}^{n-1}(t_{i+1}-t_i)^2.
\end{align*}
Now 
\begin{align*}
\expval\Big[\Big(\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})(t_{i+1}-t_i)\Big)^2\Big] & = \sum_{i=0}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^2}(t_{i+1}-t_i)^2 = \sum_{i=0}^{n-1}(t_{i+1}-t_i)^4 \leq \abs{\sigma}^3t,
\end{align*}
where the first equality holds since $\expval\parq{(W_{t_{i+1}}-W_{t_i})(W_{t_{j+1}}-W_{t_j})}=0$ for every $i\neq j$, and so
\begin{equation}\label{ex4:quad_var_conv_2}
ab\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})(t_{i+1}-t_i) \xrightarrow{L^2} 0 \qquad \text{as } \abs{\sigma}\to0.
\end{equation}
Combining (\ref{ex4:quad_var_conv}) and (\ref{ex4:quad_var_conv_2}) and the fact that $\sum_{i=0}^{n-1}(t_{i+1}-t_i)^2\leq\abs{\sigma}t\xrightarrow{L^2} 0$ as $\abs{\sigma}\to0$, we get that 
$$
Q_t^\sigma(aW_t+bt) \xrightarrow{L^2} t \qquad \text{as } \abs{\sigma}\to0.
$$

\paragraph*{(b)}

Let's consider
$$
Q_{t,4}^\sigma(W_t) = \sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^4.
$$
We have
$$
\expval\parq{(Q_{t,4}^\sigma(W_t))^2} = \sum_{i=0}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^8} + \sum_{\substack{i,j=0 \\ i\neq j}}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^4(W_{t_{j+1}}-W_{t_j})^4}
$$
Now
\begin{equation}\label{ex4.b:quad_var_conv}
\sum_{i=0}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^8} = 105\sum_{i=0}^{n-1}(t_{i+1}-t_i)^4 \leq 105\abs\sigma^3t \xrightarrow{L^2} 0 \qquad \text{as } \abs{\sigma}\to0
\end{equation}
and 
\begin{align}
\sum_{\substack{i,j=0 \\ i\neq j}}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^4(W_{t_{j+1}}-W_{t_j})^4} & = \sum_{\substack{i,j=0 \\ i\neq j}}^{n-1}\expval\parq{(W_{t_{i+1}}-W_{t_i})^4}\expval\parq{(W_{t_{j+1}}-W_{t_j})^4} \notag \\ & = 225\sum_{i=0}^{n-1}(t_{i+1}-t_i)^2\sum_{\substack{j=0 \\ j\neq i}}^{n-1}(t_{j+1}-t_j)^2  \notag \\ & \leq 225\sum_{i=0}^{n-1}(t_{i+1}-t_i)^2\abs\sigma t \notag \\ & \leq 225\abs\sigma^2t^2 \xrightarrow{L^2} 0 \qquad \text{as } \abs{\sigma}\to0.\label{ex4.b:quad_var_conv1}
\end{align}
Hence (\ref{ex4.b:quad_var_conv}) and (\ref{ex4.b:quad_var_conv1}) imply that 
$$
Q_{t,4}^\sigma(W_t) \xrightarrow{L^2} 0 \qquad \text{as } \abs{\sigma}\to0.
$$

\section*{Exercise 5}

Let us denote by $X_t^x$ the solution to $\dot{X_t}=a(X_t)$, $X_0=x$. If $f\in C^2_b(\mathbb{R})$, then
\begin{align*}
(\mathcal{L}f)(x) = \lim_{t\to 0^+}\frac{f(X_t^x) - f(x)}{t} = \frac{d}{dt}f(X_t^x)\Big|_{t=0} = f'(X_0^x)\frac{d}{dt}X_t^x\Big|_{t=0} = f'(x)a(X^x_0) = a(x)f'(x),
\end{align*}
i.e. $\mathcal{L} = a\,\partial_x$.

\section*{Exercise 6}

Given $f\in C^2_b$, we have, for $x\in K$, where $K\subset\mathbb{R}$ is a compact set such that $\mathrm{supp}\bra{p(\cdot,t|\cdot,0)}\subseteq K^2$, 
\begin{equation}\label{ex6:inf_gen}
(\mathcal{L}f)(x) = \lim_{t\to 0^+}\frac{\expval_x\parq{f(X_t)} - f(x)}{t} = \lim_{t\to 0^+}\frac{1}{t}\int_K (f(y)-f(x))\,p(y,t|x,0)\,dy. 
\end{equation}
If $\varepsilon>0$ we can split the integral in (\ref{ex6:inf_gen}) as
\begin{align*}
t^{-1}\int_K (f(y)-f(x))\,p(y,t|x,0)\,dy & = t^{-1}\int_{y\in K, \abs{x-y}\geq\varepsilon} (f(y)-f(x))\,p(y,t|x,0)\,dy \\ & + t^{-1}f'(x)\int_{y\in K, \abs{x-y}<\varepsilon} (y-x)\,p(y,t|x,0)\,dy \\ & + t^{-1}/2\int_{y\in K, \abs{x-y}<\varepsilon} f''(\xi_y)(y-x)^2\,p(y,t|x,0)\,dy,
\end{align*}
where $\xi_y$ is a point between $x$ and $y$. Now,
$$
t^{-1}\int_{y\in K, \abs{x-y}\geq\varepsilon} (f(y)-f(x))\,p(y,t|x,0)\,dy \quad  \xrightarrow[t\to0^+] \quad \int_{y\in K, \abs{x-y}\geq\varepsilon} (f(y)-f(x))\,W(y|x)\,dy + O(\varepsilon),
$$
$$
t^{-1}f'(x)\int_{y\in K, \abs{x-y}<\varepsilon} (y-x)\,p(y,t|x,0)\,dy \quad  \xrightarrow[t\to0^+] \quad f'(x)O(\varepsilon), 
$$
\begin{multline*}
t^{-1}/2\int_{y\in K, \abs{x-y}<\varepsilon} f''(\xi_y)(y-x)^2\,p(y,t|x,0)\,dy \leq \\ \leq \norm{f''}_\infty t^{-1}/2\int_{y\in K, \abs{x-y}<\varepsilon} (y-x)^2\,p(y,t|x,0)\,dy \quad  \xrightarrow[t\to0^+] \quad \norm{f''}_\infty O(\varepsilon)/2.
\end{multline*}
hence, if we send $\varepsilon\to 0^+$, we get
$$
(\mathcal{L}f)(x) = \int_{K} (f(y)-f(x))\,W(y|x)\,dy = \int_\mathbb{R} f(y)\,W(y|x)\,dy - f(x)\lambda(x).
$$

\end{document}